{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyHxqRVIOatZ"
      },
      "source": [
        "# **Example using GPT Wrapper for KMS Technology Technical Challenge**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57OSvu2TOoCW"
      },
      "source": [
        "## 1. Clone wrapper repo from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPyRuGacMO2x",
        "outputId": "44fdb9d6-4653-4fc8-8bee-1405e6071b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-wrapper'...\n",
            "remote: Enumerating objects: 141, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 141 (delta 65), reused 112 (delta 36), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (141/141), 16.88 MiB | 14.13 MiB/s, done.\n",
            "Resolving deltas: 100% (65/65), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tsdocode/gpt-wrapper.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd-x2SP4MTC0",
        "outputId": "8b044163-6455-46e1-f2eb-cdf5bcd25e08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gpt-wrapper\n"
          ]
        }
      ],
      "source": [
        "cd gpt-wrapper/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUOCYlfuMVNW",
        "outputId": "fcc37de0-8a9f-461c-9dba-241e809e6148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset.py   model.py   requirements.txt  utils.py\n",
            "__init__.py  README.md  trainer.py\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_c6jdSGOwVJ"
      },
      "source": [
        "## 2. Install requirements python package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d4e0BJcMWT_"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Reksw9nyO6w4"
      },
      "source": [
        "## 3. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DytNlDNyO6mr"
      },
      "source": [
        "Generate sample dataset, skip this step if you already have dataset in json format\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UFFZW5t3Me01"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "\n",
        "sample_data = {\n",
        "    'data' : [\n",
        "          {\n",
        "              'schema': 'table(name : text, score: number )',\n",
        "              'question': 'who have score over 9',\n",
        "              'sql': 'SELECT name FROM TABLE WHERE score > 9'\n",
        "          }\n",
        "    ]\n",
        "}\n",
        "\n",
        "with open('sample_data.json' , 'w') as f:\n",
        "  json.dump(sample_data, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tteyOCq5PFB0"
      },
      "source": [
        "Convert json format in to trainable txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbdsn77fMyp9",
        "outputId": "f05c0fee-2b47-4046-ac5b-db6faabf9c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "\r  0% 0/1 [00:00<?, ?it/s]\r100% 1/1 [00:00<00:00, 14926.35it/s]\n"
          ]
        }
      ],
      "source": [
        "!python dataset.py -i sample_data.json -o train_data.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gw31uRxM4Bm",
        "outputId": "45ee0e06-ddc1-4837-a98e-712e87781531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dataset.py   model.py   requirements.txt  train_data.txt  utils.py\n",
            "__init__.py  README.md  sample_data.json  trainer.py\n"
          ]
        }
      ],
      "source": [
        "ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9uEcls9PKZa"
      },
      "source": [
        "Train model with txt dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vri4kMxeM6Mj",
        "outputId": "bbf6a45a-419f-4bef-bd4e-6c37f74864fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model version: 125M\n",
            "Epochs: 3\n",
            "Learning rate: 5e-05\n",
            "Output folder: test\n",
            "Loading model...\n",
            "03/23/2022 05:29:49 - INFO - happytransformer.happy_transformer -   Using model: cpu\n",
            "Done!\n",
            "Training model...\n",
            "03/23/2022 05:29:49 - INFO - happytransformer.happy_transformer -   Preprocessing dataset...\n",
            "Downloading and preparing dataset text/default to /root/.cache/huggingface/datasets/text/default-e6c5b50760abf3d0/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 5924.16it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 780.48it/s]\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-e6c5b50760abf3d0/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 570.81it/s]\n",
            "100% 1/1 [00:00<00:00, 215.27ba/s]\n",
            "100% 1/1 [00:00<00:00, 375.77ba/s]\n",
            "03/23/2022 05:29:50 - INFO - happytransformer.happy_transformer -   Training...\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 1\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 3\n",
            "100% 3/3 [00:06<00:00,  2.11s/it]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 6.5013, 'train_samples_per_second': 0.461, 'train_steps_per_second': 0.461, 'train_loss': 3.676136334737142, 'epoch': 3.0}\n",
            "100% 3/3 [00:06<00:00,  2.17s/it]\n",
            "Finished training model, you can save it now using model.save()\n",
            "Configuration saved in ./saved/test/config.json\n",
            "Model weights saved in ./saved/test/pytorch_model.bin\n",
            "tokenizer config file saved in ./saved/test/tokenizer_config.json\n",
            "Special tokens file saved in ./saved/test/special_tokens_map.json\n"
          ]
        }
      ],
      "source": [
        "!python trainer.py -i train_data.txt -m 125M -e 3 -o test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_-veRacPtd1"
      },
      "source": [
        "## 4. Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJ8SmpeCNoKT",
        "outputId": "8ac9a190-5c50-447d-f3ca-d863ef2ee3bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "03/23/2022 05:31:57 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from model import GPTModel\n",
        "\n",
        "test_model = GPTModel(model_path='./saved/test')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS6vz2gQPxaM"
      },
      "source": [
        "## 5. Test model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "asdmOZs1QzHp"
      },
      "outputs": [],
      "source": [
        "#Define custom preprocess and postprocess\n",
        "#You can also edit default function in utils.py\n",
        "def preprocessing(schema, question):\n",
        "    #Feel free to custom input prompt\n",
        "    prompt = f\"schema: {schema} | question: {question} | sql:\"\n",
        "    return prompt\n",
        "\n",
        "def postprocessing(result):\n",
        "    return result.split('|')[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bsy56wuXOPg1",
        "outputId": "fef5a2c6-a114-40a4-d632-78b2ad316db1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " select name from table where score > 8 \n"
          ]
        }
      ],
      "source": [
        "schema = \"table(name : text, score: number )\"\n",
        "question = \"show name have score over 8\"\n",
        "\n",
        "SQL = test_model.generate(schema, question, preprocessing, postprocessing)\n",
        "print(SQL)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
